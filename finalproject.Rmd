---
title: |
  |
  Hospital Insurance Charges Model
  \vspace{0.5cm}\
  Final for STAT 425
author: 
  - "Nick Xing"
  - "Zihan Zhu"
  - "Noah Clark"
date: "May 6, 2024"
output: 
  pdf_document:
    toc: yes
    toc_depth: 3
    keep_tex: yes  # This keeps the intermediate LaTeX file for debugging
fontsize: 12pt
mainfont: Arial
---

```{r include=FALSE}
library(ggplot2)
library(gridExtra)
# Reading the data from the file
insurance = read.csv("insurance.csv")
attach(insurance)
insmod = lm(charges~., data = insurance)
```


\newpage
##  Section 1: Introduction
This project focuses on the analysis of medical insurance data, particularly focusing on the medical charges acquired from hospital visits. The response variable is the variable charges from the insurance data. This data is found on Kaggle, as was last updated 6 years ago, in 2018. 
  In this data set, there 6 variables excluding the response variable, charges, which shows the exact charges the patient acquired from their hospital visit. Each variable is defined within the Kaggle (2018) website and explained in this order:

* age: Age of primary beneficiary

* sex: Insurance contractor gender (female or male)

* bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,
objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9

* children: Number of children covered by health insurance / Number of dependents

* smoker: Smoking (yes or no)

* region: Beneficiary's residential area in the US (northeast, southeast, southwest, northwest)

* charges: Individual medical costs billed by health insurance


This report compares multiple models derived from analysis of the data set including an Explanatory Model, a Linear Regression Model, a Regression Tree, and a Random Forest.
  All analyses are performed in the R environment. This is possible to recreate with the code provided in this file.         Setting seeds might yield slightly different results; however, we did not use a seed to analyze this data. No function (i.e. _set.seed_) should change the results concluded from this data, although it is possible to slightly alter the data using said functions. Again, the results would be concluded the same way.
  This report has 4 sections. Section 1 is this introduction. Section 2 uses modelling assumptions and models to analyze the data and provide insights on the data. Section 3 presents predictive models and shows training from said models. Section 4 compares models and provides a discussion of these results.

\newpage
## Section 2: Exploratory Data Analysis
Here is a brief overview of all of the variables of this dataset:

Numerical Variables:

- age: age of primary beneficiary
- bmi: body mass index
- children: number of children/dependents covered by health insurance
- charges (response variable): medical costs billed by health insurance

Categorical Variables:

- sex: male or female
- smoker: yes or no
- region: residential area in the US, can be northeast, southeast, southwest, or northwest

```{r include=FALSE}
# Checking high leverage
n=1338; p=8;
lev=influence(insmod)$hat
lev[lev>2*p/n]
# Checking for unusual observations (outliers)
stud <- rstudent(insmod)
```

The first step is to check for unusual observations by performing an outlier test. To do so, we obtained the studentized residuals for our data, and compared the largest values with the Bonferroni critical value.

```{r include=FALSE}
qt(.05/(2*n), n-p-1) # Bonferroni critical value
```

The Bonferroni critical value we calculated is -4.137174. Any observation with studentized residual higher than the absolute value of it will be considered an outlier.

Next, here are the five largest studentized residuals, which we will be comparing to the Bonferroni critical value:

```{r echo=FALSE}
sort(abs(stud), decreasing=TRUE)[1:5] # largest 5 studentized residuals
# 1301 and 578 are unusual observations, we should remove them
# leverage values for highly influential points (?)
# Cook's distance
```

Since the absolute value of their respective studentized residual is greater than the Bonferroni critical value, we can conclude that Observation #1301 and Observation #578 are outliers.

```{r include=FALSE}
# Checking for highly influential points (Cook's distance)
cook = cooks.distance(insmod)
max(cook)
```

We also checked Cook's distance, however we found that there is no point with Cook's distance greater than 1. We will simply remove the two outliers we found earlier.

```{r include=FALSE}
insurance2 = insurance[-c(1301,578),]
attach(insurance2)
insmod2 = lm(charges~., data = insurance2) # new model with the two outliers removed
summary(insmod2)
```

\newpage
After removing these two points, we will be analyzing the predictors to see if there are any variables we should add or remove.

```{r echo=FALSE}
colors <- c("red", "green", "blue", "orange", 'yellow', 'purple')
p1=ggplot(insurance2, aes(age, charges)) + geom_point(size=0.5) 
p2=ggplot(insurance2, aes(as.factor(sex), charges)) + geom_boxplot(outlier.size=0.5, fill = c('pink', 'blue'))
p3=ggplot(insurance2, aes(bmi, charges)) + geom_point(size=0.5)
p4=ggplot(insurance2, aes(as.factor(children), charges)) + geom_boxplot(outlier.size=0.5, fill = c("red", "green", "blue", "orange", 'yellow', 'purple'))
p5=ggplot(insurance2, aes(as.factor(smoker), charges)) + geom_boxplot(outlier.size=0.5, fill = c('green', 'red'))
p6=ggplot(insurance2, aes(as.factor(region), charges)) + geom_boxplot(outlier.size=0.5, fill = c('red', 'blue', 'green', 'yellow')) + theme(axis.text.x = element_text(angle = 45, hjust = 1))
grid.arrange(p1,p2,p3,p4,p5,p6,nrow=2, ncol=3, bottom="Figure 1: Plot of Charges vs All Predictors")
```

From the plot of charges vs age, we notice a trend that the charges increase as age increases. We will not be removing 'age' as a predictor since it is clear that it has a significant impact on the response variable 'charges'. This makes sense because as people grow older, their health declines, and the insurance costs increase.

The charges vs sex boxplot tells us that males tend to have higher charges than females. We will conduct a t-test to test this difference.

```{r include=FALSE}
t.test(charges ~ sex, data=insurance2, var.equal=TRUE)[3] # p-value of 'sex'
```

From the t-test, we obtained a p-value of about 0.0346.

Given that the p-value is less than 0.05, we conclude that 'sex' is a significant predictor, with males incurring higher insurance costs than females. A possible explanation for this is that males are usually more at risk of health conditions that result in increased charges compared to females.

From the plot of charges vs bmi, we can see a general trend that as bmi increases, the charges also increase. As a result, we will not be removing 'bmi' as a predictor since it appears to have an impact on the response variable. The graph also makes sense logically since people with a higher bmi tend to be overweight and subsequently, have worse health than people with a lower bmi, causing their insurance charges to be greater as well.

The boxplot of charges vs children conveys to us that charges tend to increase as the number of children grows from 0 or 1 to 2 or 3. While there are some data points for people with 4 or 5 children, it appears that these groups have a very low amount of people, and it may be challenging to draw meaningful conclusions from it. We perform an ANOVA test to test the significance of 'children' as a predictor.

```{r include=FALSE}
insmodchildren = lm(charges ~ children, data=insurance2)
anova(insmodchildren)[1,5] # p-value of 'children'
```

We obtained a p-value of approximately 0.00862.

This p-value is less than 0.05, thus signifying that the predictor 'children' is significant. We will be keeping as a predictor in our model. A larger number of children/dependents covered leads to higher charges, as there are more individuals who need to be covered.

From the boxplot of charges vs smoker, it is evident that smokers generally face significantly higher charges than non-smokers. Consequently, we will not be removing 'smoker' as a predictor. People who smoke typically incur higher insurance costs due to the health risks posed by smoking.

The boxplot of charges vs region shows us that the southeast region seemingly has higher charges than the other three regions. We will conduct an ANOVA test to make sure that the predictor 'region' is significant.

```{r include=FALSE}
insmodreg = lm(charges ~ region, data=insurance2)
anova(insmodreg)[1,5] # p-value of 'region'
```

From the ANOVA test, we obtained a p-value of about 0.0453.

The p-value is less than 0.05, indicating that the predictor 'region' is significant, and thus we will retain it as a predictor. This confirms that there are regional differences in the insurance costs.

\newpage

Now, we will explore whether or not we should include any interactions between categorical variables. We will be analyzing all possible first-order and second-order interactions between our categorical predictors, including:

- 'sex' and 'smoker'
- 'sex' and 'region'
- 'region' and 'smoker'
- 'sex', 'smoker', and 'region'

Here are some plots that show the interactions between these predictors.

```{r echo=FALSE}
p_int1=ggplot(insurance2, aes(x = paste(sex, smoker), y = charges)) + geom_boxplot(outlier.size=0.5, fill = c('purple','pink', 'red', 'blue')) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(aspect.ratio=1)
p_int2=ggplot(insurance2, aes(x = paste(sex, region), y = charges)) + geom_boxplot(outlier.size=0.5, fill =  c("red", "green", "blue", "orange", 'yellow', 'purple', 'brown', 'pink')) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(aspect.ratio=1)
p_int3=ggplot(insurance2, aes(x = paste(region, smoker), y = charges)) + geom_boxplot(outlier.size=0.5, fill =  c("red", "green", "blue", "orange", 'yellow', 'purple', 'brown', 'pink')) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + theme(aspect.ratio=1)
grid.arrange(p_int1,p_int2,p_int3, ncol=3, bottom="Figure 2: Plot of Charges vs Interactions Between Categorical Variables")
```

From the plots, it appears as though smoking increases the charges more for males than females. Additionally, it seems like smoking has a greater impact on charges in the southeast and southwest, when compared to northeast and northwest. However, it does not appear that sex and region is a significant interaction.

```{r include=FALSE}
mod1 = lm(charges ~ sex*smoker*region, data=insurance2)
anova(mod1)
# we found that sex:smoker:region is not significant, so we remove that and test the first-order interactions
```

```{r include=FALSE}
mod2 = lm(charges ~ sex*smoker + sex*region + smoker*region, data=insurance2)
anova(mod2)
# we find that sex:region is not significant and remove that
```

```{r include=FALSE}
mod3 = lm(charges ~ sex*smoker + smoker*region, data=insurance2)
anova(mod3)
# we find that both sex:smoker and smoker:region are significant interactions, so we keep them in the final model
```

We applied sequential F-tests with the anova function to assess the significance of the interaction terms in our model. Initially, we included all potential interactions among the three variables 'sex', 'smoker', and 'region'. We first found that the second-order interaction sex:smoker:region was not significant, and removed it from the model. Subsequently, we tested the first-order interactions and found that the interaction sex:region was not significant. Finally, after removing sex:region as a predictor, we concluded that only the interactions sex:smoker and smoker:region are statistically significant, and we will retain them in the final model.

```{r include=FALSE}
insmodfinal = lm(charges~.+sex:smoker+region:smoker, data=insurance2)
summary(insmodfinal)
```

Now, we will examine the numerical variables and see if there is evidence of any nonlinear trends.

```{r echo=FALSE}
mod4 = lm(charges~age, data=insurance2)
mod4_quad = lm(charges~age+I(age^2), data=insurance2)
mod4_plot = ggplot(insurance2, aes(age, charges)) + geom_point(size=0.5) + 
  geom_line(aes(y=fitted(mod4_quad), color = 'quadratic fit'))
mod5 = lm(charges~bmi, data=insurance2)
mod5_quad = lm(charges~bmi+I(bmi^2), data=insurance2)
mod5_plot = ggplot(insurance2, aes(bmi, charges)) + geom_point(size=0.5) + 
  geom_line(aes(y=fitted(mod5_quad), color = 'quadratic fit'))
mod6 = lm(charges~children, data=insurance2)
mod6_quad = lm(charges~children+I(children^2), data=insurance2)
mod6_plot = ggplot(insurance2, aes(children, charges)) + geom_point(size=0.5) + 
  geom_line(aes(y=fitted(mod6_quad), color = 'quadratic fit'))
grid.arrange(mod4_plot, mod5_plot, mod6_plot, nrow=2, ncol=2, bottom="Figure 3: Plot of Quadratic Fit for Charges vs Numerical Predictors")
```


```{r echo = FALSE}
mod4_mse <- (mean((mod4[['residuals']])^2))
mod5_mse <- (mean((mod5[['residuals']])^2))
mod6_mse <- (mean((mod6[['residuals']])^2))
mod4_rmse <- sqrt(mod4_mse)
mod5_rmse <- sqrt(mod5_mse)
mod6_rmse <- sqrt(mod6_mse)
```


Based on the plot of the quadratic fit for charges vs age, we see there is evidence of non-linearity for the predictor 'age' since the quadratic fit has some upward curvature.

Similarly, based on the plot of the quadratic fit for charges vs bmi, we see there is some evidence of non-linearity for the predictor 'bmi', although maybe not as clear as for 'age'. This time, the quadratic fit curves downwards.

Finally, the plot of charges vs children also suggests some non-linearity. However, the non-linearity is not as evident as for 'age and 'bmi' since although the quadratic fit does appear to curve downwards as the number of children increases, there is not many data points for 4 and 5 children, so we may not be able to draw a strong conclusion.

\newpage
## Section 3: Methodology
You are required to build at least two prediction models using the methods covered in this class. For each model or method you are using, include a brief description of the methodology and a description of the R implementation (R coding steps).

Your should consider the following sub-sections:
Section 3.1: Start with a simple model, a model that doesn't require much training, for example, a linear regression model built after appropriate variable selection and model diagnostics.
Section 3.2: Use the Linear Regression model built in 1 to make predictions on a testing set.
Section 3.3: Fit a different kind of model like a non-parametric regression, regularized regression models or others, and make a prediction on the same testing set.
Section 3.4 (Optional): You can also try with other methods learnt in other classes if applicable (like for example Random Forests).

\newpage
## Section 4: Discussion and Conclusions
In this part you should make a summary of your results  by comparing the different modeling approaches and discussing the impact of your analysis. You should also write the main conclusions of your analysis in three to four bullet points.
